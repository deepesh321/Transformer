{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjebmphL4BGO3uTpEfTyre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepesh321/Transformer/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ0gBho12xTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import spacy\n",
        "import re\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import os\n",
        "import dill as pickle\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW1hagkH4gSl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcb69964-1890-46f6-b180-57112b89c791"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxeTEA-U42-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/content/drive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vB0H4HmjSil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "c39c2129-d616-4570-89de-ad52dd1b4260"
      },
      "source": [
        "!spacy download en && spacy download fr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcs3bV5F5BiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /content/drive/'My Drive'/'Colab Notebooks'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrlbTGAnKVZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args:\n",
        "  src_data = '/content/drive/My Drive/Colab Notebooks/english.txt'\n",
        "  trg_data = '/content/drive/My Drive/Colab Notebooks/french.txt'\n",
        "  src_lang='en'\n",
        "  trg_lang='fr'\n",
        "  no_cuda=False\n",
        "  SGDR=False\n",
        "  epochs=20\n",
        "  d_model=256\n",
        "  n_layers=3\n",
        "  heads=4\n",
        "  dropout=0.1\n",
        "  batchsize=100\n",
        "  printevery=100\n",
        "  lr=0.0001\n",
        "  max_strlen=80\n",
        "  load_weights=None\n",
        "\n",
        "opt=Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO6zKPN-3FBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.embed=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embed(x)\n",
        "\n",
        "class positional_encoding(nn.Module):\n",
        "  def __init__(self,d_model,max_len=200,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    pe=torch.zeros(max_len,d_model)  #positional matrix\n",
        "    for pos in range(max_len):\n",
        "      for i in range(0,d_model,2):\n",
        "        pe[pos,i]=math.sin(pos/(10000**(2*i/d_model)))\n",
        "        if i+1< d_model:\n",
        "          pe[pos,i+1]=math.cos(pos/(10000**(2*(i+1)/d_model)))\n",
        "\n",
        "    pe=pe.unsqueeze(0)\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x*math.sqrt(self.d_model)  #make embedding relatively larger\n",
        "    seq_len=x.size(1)\n",
        "    pe=Variable(self.pe[:,:seq_len],requires_grad=False)\n",
        "    if x.is_cuda:\n",
        "      pe.to('cuda')\n",
        "    x=x+pe\n",
        "    return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFUPL2wNJ9Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class tokenize(object):\n",
        "\n",
        "  def __init__(self,lang):\n",
        "    self.nlp=spacy.load(lang)\n",
        "\n",
        "  def tokenizer(self,sentence):\n",
        "    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "    sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "    sentence = sentence.lower()\n",
        "    return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9SbjoDUztb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(opt):\n",
        "  if opt.src_data is not None:\n",
        "    try:\n",
        "      opt.src_data=open(opt.src_data).read().strip().split('\\n')\n",
        "    except:\n",
        "      print(\"error: '\" + opt.src_data + \"' file not found\")\n",
        "      quit()\n",
        "\n",
        "  if opt.trg_data is not None:\n",
        "    try:\n",
        "      opt.trg_data=open(opt.trg_data).read().strip().split('\\n')\n",
        "    except:\n",
        "      print(\"error: '\" + opt.trg_data + \"' file not found\")\n",
        "      quit()\n",
        "\n",
        "\n",
        "def create_fields(opt):\n",
        "  spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n",
        "  if opt.src_lang not in spacy_langs:\n",
        "    print('invalid src language: ' + opt.src_lang + 'supported languages : ' + spacy_langs)  \n",
        "  if opt.trg_lang not in spacy_langs:\n",
        "    print('invalid src language: ' + opt.trg_lang + 'supported languages : ' + spacy_langs)  \n",
        "\n",
        "  print('loading spacy tokenizer')\n",
        "  t_src=tokenize(opt.src_lang)\n",
        "  t_trg=tokenize(opt.trg_lang)\n",
        "\n",
        "  TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "  SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
        "\n",
        "  if opt.load_weights is not None:\n",
        "    try:\n",
        "      print(\"loading presaved fields...\")\n",
        "      SRC = pickle.load(open(f'{opt.load_weights}/SRC.pkl', 'rb'))\n",
        "      TRG = pickle.load(open(f'{opt.load_weights}/TRG.pkl', 'rb'))\n",
        "    except:\n",
        "      print(\"error opening SRC.pkl and TXT.pkl field files, please ensure they are in \" + opt.load_weights + \"/\")\n",
        "      quit()\n",
        "        \n",
        "  return(SRC, TRG)\n",
        "\n",
        "\n",
        "def create_dataset(opt,SRC,TRG):\n",
        "\n",
        "  print('Create dataset and iterator...')\n",
        "  raw_data={'src':[line for line in opt.src_data],'trg':[line for line in opt.trg_data]}\n",
        "  df=pd.DataFrame(raw_data,columns=['src','trg'])\n",
        "\n",
        "  mask=(df['src'].str.count(' ')< opt.max_strlen) & (df['trg'].str.count(' ')< opt.max_strlen)\n",
        "  # print(mask)\n",
        "  df=df.loc[mask]\n",
        "\n",
        "  df.to_csv('temp.csv',index=False)\n",
        "  data_fields=[('src',SRC),('trg',TRG)]\n",
        "\n",
        "  train = data.TabularDataset('temp.csv', format='csv', fields=data_fields)\n",
        "  # print(train)\n",
        "\n",
        "  train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
        "    \n",
        "  # os.remove('temp.csv')\n",
        "\n",
        "  if opt.load_weights is None:\n",
        "    SRC.build_vocab(train)\n",
        "    TRG.build_vocab(train)\n",
        "    if opt.checkpoint > 0:\n",
        "      try:\n",
        "        os.mkdir(\"weights\")\n",
        "      except:\n",
        "        print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
        "        quit()\n",
        "      pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
        "      pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
        "\n",
        "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
        "    # print(opt.src_pad)\n",
        "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "    opt.train_len = get_len(train_iter)\n",
        "\n",
        "    return train_iter\n",
        "\n",
        "def get_len(train):\n",
        "\n",
        "  for i, b in enumerate(train):\n",
        "    pass\n",
        "    \n",
        "  return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv8kTukKodSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "  def __init__(self,d_model,eps=1e-6):\n",
        "    super().__init__()\n",
        "\n",
        "    self.size=d_model\n",
        "    self.alpha=nn.Parameter(torch.ones(self.size))\n",
        "    self.bias=nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "    self.eps=eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    norm = self.alpha * (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "    return norm\n",
        "\n",
        "\n",
        "def attention(q,k,v,d_k,mask=None,dropout=None):\n",
        "\n",
        "  scores= torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    mask=mask.unsqueeze(1)\n",
        "    scores=scores.masked_fill(mask==0,-1e9)\n",
        "\n",
        "  scores=F.softmax(scores,dim=1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    scores=dropout(scores)\n",
        "\n",
        "  output= torch.matmul(scores,v)\n",
        "  return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,heads,d_model,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model=d_model\n",
        "    self.d_k=d_model//heads\n",
        "    self.h=heads\n",
        "\n",
        "    self.q_linear=nn.Linear(d_model,d_model)\n",
        "    self.v_linear=nn.Linear(d_model,d_model)\n",
        "    self.k_linear=nn.Linear(d_model,d_model)\n",
        "    \n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.out=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "\n",
        "    bs=q.size(0)\n",
        "    k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "    q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "    v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "    k = k.transpose(1,2)\n",
        "    q = q.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    scores=attention(q,k,v,self.d_k,mask,self.dropout)\n",
        "    concat=scores.transpose(1,2).contiguous().view(bs,-1,self.d_model)\n",
        "\n",
        "    output=self.out(concat)\n",
        "    return output\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff=2048,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1=nn.Linear(d_model,d_ff)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.linear_2=nn.Linear(d_ff,d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCqIrg8x3ic7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,heads,dropout=0.1):\n",
        "\n",
        "    super().__init__()\n",
        "    self.norm_1=Norm(d_model)\n",
        "    self.norm_2=Norm(d_model)\n",
        "    self.attn=MultiHeadAttention(heads,d_model,dropout=dropout)\n",
        "    self.ff = FeedForward(d_model, dropout=dropout)\n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "\n",
        "    x2=self.norm_1(x)\n",
        "    x=x+self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "    x2=self.norm_2(x)\n",
        "    x=x+self.dropout_2(self.ff(x2))\n",
        "    return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,heads,dropout=0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm_1 = Norm(d_model)\n",
        "    self.norm_2 = Norm(d_model)\n",
        "    self.norm_3 = Norm(d_model)\n",
        "    \n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "    self.dropout_3 = nn.Dropout(dropout)\n",
        "    \n",
        "    self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "    self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "    self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "  def forward(self,x,e_outputs,src_mask,trg_mask):\n",
        "\n",
        "    x2=self.norm_1(x)\n",
        "    x=x+self.dropout_1(self.attn_1(x2,x2,x2,trg_mask))\n",
        "    x2=self.norm_2(x)\n",
        "    x=x+self.dropout_2(self.attn_2(x2,e_outputs,e_outputs,src_mask))\n",
        "    x2=self.norm_3(x)\n",
        "    x = x + self.dropout_3(self.ff(x2))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqligVsQCEZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clones(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,N,heads,dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.N=N\n",
        "    self.embed = embedding(vocab_size,d_model)\n",
        "    self.pe = positional_encoding(d_model,dropout=dropout)\n",
        "    self.layers=get_clones((EncoderLayer(d_model, heads, dropout)), N)\n",
        "    self.norm=Norm(d_model)\n",
        "\n",
        "  def forward(self,src,mask):\n",
        "    x=self.embed(src)\n",
        "    x=self.pe(x)\n",
        "    for i in range(self.N):\n",
        "      x=self.layers[i](x,mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,N,heads,dropout):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.N=N\n",
        "    self.embed=embedding(vocab_size,d_model)\n",
        "    self.pe = positional_encoding(d_model,dropout=dropout)\n",
        "    self.layers=get_clones((DecoderLayer(d_model, heads, dropout)), N)\n",
        "    self.norm=Norm(d_model)\n",
        "\n",
        "  def forward(self,trg,e_outputs,src_mask,trg_mask):\n",
        "    x=self.embed(trg)\n",
        "    x=self.pe(x)\n",
        "    for i in range(self.N):\n",
        "      x=self.layers[i](x,e_outputs,src_mask,trg_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,src_vocab,trg_vocab,d_model,N,heads,dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder=Encoder(src_vocab,d_model,N,heads,dropout)\n",
        "    self.decoder=Decoder(trg_vocab,d_model,N,heads,dropout)\n",
        "    self.out=nn.Linear(d_model,trg_vocab)\n",
        "\n",
        "  def forward(self,src,trg,src_mask,trg_mask):\n",
        "\n",
        "    e_outputs = self.encoder(src, src_mask)\n",
        "    d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "    output = self.out(d_output)\n",
        "    return output\n",
        "\n",
        "def get_model(opt, src_vocab, trg_vocab):\n",
        "\n",
        "  assert opt.d_model%opt.heads==0\n",
        "  assert opt.dropout < 1\n",
        "\n",
        "  model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n",
        "\n",
        "  if opt.load_weights is not None:\n",
        "    print('Loading pretrained weights')\n",
        "    model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
        "\n",
        "  else:\n",
        "    for p in model.parameters():\n",
        "      if p.dim()>1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "  if opt.device==0:\n",
        "    model=model.cuda()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRhbOTrT5MKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nopeak_mask(size,opt):\n",
        "  np_mask=np.triu(np.ones((1,size,size)),k=1).astype('uint8')\n",
        "  np_mask=Variable(torch.from_numpy(np_mask)==0)\n",
        "  if opt.device==0:\n",
        "    np_mask=np_mask.cuda()\n",
        "  return np_mask\n",
        "\n",
        "def create_masks(src,trg,opt):\n",
        "  src_mask=(src!=opt.src_pad).unsqueeze(-2)\n",
        "\n",
        "  if trg is not None:\n",
        "    trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
        "    size=trg.size(1)\n",
        "    np_mask=nopeak_mask(size,opt)\n",
        "    if trg.is_cuda:\n",
        "      np_mask.cuda()\n",
        "    trg_mask=trg_mask & np_mask\n",
        "\n",
        "  else:\n",
        "    trg_mask=None\n",
        "  return src_mask,trg_mask\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "  def create_batches(self):\n",
        "    if self.train:\n",
        "      def pool(d, random_shuffler):\n",
        "        for p in data.batch(d, self.batch_size * 100):\n",
        "          p_batch = data.batch(sorted(p, key=self.sort_key),self.batch_size, self.batch_size_fn)\n",
        "          for b in random_shuffler(list(p_batch)):\n",
        "            yield b\n",
        "      self.batches = pool(self.data(), self.random_shuffler)\n",
        "          \n",
        "    else:\n",
        "      self.batches = []\n",
        "      for b in data.batch(self.data(), self.batch_size,self.batch_size_fn):\n",
        "        self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "  global max_src_in_batch, max_tgt_in_batch\n",
        "  if count == 1:\n",
        "    max_src_in_batch = 0\n",
        "    max_tgt_in_batch = 0\n",
        "  max_src_in_batch = max(max_src_in_batch,len(new.src))\n",
        "  max_tgt_in_batch = max(max_tgt_in_batch,len(new.trg) + 2)\n",
        "  src_elements = count * max_src_in_batch\n",
        "  tgt_elements = count * max_tgt_in_batch\n",
        "  return max(src_elements, tgt_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n256-A6mdas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "75145622-97a1-4fad-a26e-012bbbb487ab"
      },
      "source": [
        "opt.device=-1\n",
        "opt.checkpoint=0\n",
        "read_data(opt)\n",
        "SRC, TRG = create_fields(opt)\n",
        "opt.train = create_dataset(opt, SRC, TRG)\n",
        "model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
        "opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading spacy tokenizer\n",
            "Create dataset and iterator...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZrPYD1lYtjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "model.train()\n",
        "    \n",
        "start = time.time()\n",
        "temp = start\n",
        "\n",
        "total_loss = 0\n",
        "\n",
        "for epoch in range(opt.epochs):\n",
        "    \n",
        "  for i, batch in enumerate(opt.train):\n",
        "    src = batch.src.transpose(0,1)\n",
        "    trg = batch.trg.transpose(0,1)\n",
        "    \n",
        "    trg_input = trg[:, :-1]\n",
        "    targets = trg[:, 1:].contiguous().view(-1)\n",
        "    \n",
        "    \n",
        "    # create function to make masks using mask code above\n",
        "    \n",
        "    src_mask, trg_mask = create_masks(src, trg_input,opt)\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    opt.optimizer.zero_grad()\n",
        "    \n",
        "    loss = F.cross_entropy(preds.view(-1, preds.size(-1)),targets, ignore_index=opt.trg_pad)\n",
        "    loss.backward()\n",
        "    opt.optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    if (i + 1) % opt.printevery == 0:\n",
        "      loss_avg = total_loss / opt.printevery\n",
        "      print(\"time = %dm, epoch %d, iter = %d, loss = %.3f,%ds per %d iters\" % ((time.time() - start) // 60,epoch + 1, i + 1, loss_avg, time.time() - temp,opt.printevery))\n",
        "      total_loss = 0\n",
        "      temp = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLGJBKqSQVaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(model, src, max_len = 80):\n",
        "    \n",
        "    model.eval()\n",
        "    src = tokenize_en(src)\n",
        "    sentence=Variable(torch.LongTensor([[EN_TEXT.vocab.stoi[tok] for tok in sentence]])).cuda()\n",
        "    src_mask = (src != input_pad).unsqueeze(-2)\n",
        "    e_outputs = model.encoder(src, src_mask)\n",
        "    outputs = torch.zeros(max_len).type_as(src.data)\n",
        "    outputs[0] = torch.LongTensor([FR_TEXT.vocab.stoi['<sos>']])\n",
        "            \n",
        "    for i in range(1, max_len):    \n",
        "      trg_mask = np.triu(np.ones((1, i, i),\n",
        "      k=1).astype('uint8')\n",
        "      trg_mask= Variable(torch.from_numpy(trg_mask) == 0).cuda()\n",
        "\n",
        "      out = model.out(model.decoder(outputs[:i].unsqueeze(0),e_outputs, src_mask, trg_mask))\n",
        "      out = F.softmax(out, dim=-1)\n",
        "      val, ix = out[:, -1].data.topk(1)\n",
        "\n",
        "      outputs[i] = ix[0][0]\n",
        "      if ix[0][0] == FR_TEXT.vocab.stoi['<eos>']:\n",
        "        break\n",
        "    return ' '.join([FR_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}